library(pacman)
# Cargar las librerías listadas e instalarlas en caso de ser necesario
p_load(tidyverse, # Manipular dataframes
stringi, # Manipular cadenas de texto
rio, # Importar datos fácilmente
sf, # Leer/escribir/manipular datos espaciales
tidymodels, # entrenamiento de modelos
spatialsample, # Muestreo espacial para modelos de aprendizaje automático
rsample, # Resamplear los datos
dplyr,
parsnip, # elastic net
dials, # elastic net tunning
recipes, # Recetas
workflows,# Crear worklows RN
metrics,# Evaluación de metricas para Ml
tidymodels,#modelos
randomforest,
ranger,
rlang,
tune,
adabag)
load("temporal.RData")
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
load("temporal.RData")
# ADABOOST M1 -----------------------------------------------------------------
AdaBase <- df %>% filter(grupo == "train") %>% select(all_of(vars))
# RANDOM FOREST-----------------------------------------------------------------
vars<- c("price", "year", "surface_imputado", "rooms",
"bedrooms", "bathrooms", "property_type",
"cocina_lujo", "cocina_estandar", "parqueadero", "terraza",
"sala_comedor", "patio_lavanderia", "walkin_closet", "estudio",
"closet", "saloncomunal_recepcion", "seguridad", "piso",
"lujos", "remodelado",  "distancia_commercial",
"distancia_bank", "distancia_bus_station",
"distancia_cafe",  "distancia_college",
"distancia_hospital", "distancia_marketplace")
# ADABOOST M1 -----------------------------------------------------------------
AdaBase <- df %>% filter(grupo == "train") %>% select(all_of(vars))
rec_ada <- recipe(price ~ ., data = AdaBase) %>%
step_unknown(all_nominal_predictors()) %>%
step_impute_mean(all_numeric_predictors()) %>%
step_novel(all_nominal_predictors()) %>%
step_zv(all_predictors())
prepped_ada <- prep(rec_ada, training = AdaBase)
AdaBase <- bake(prepped_ada, new_data = AdaBase)
# 1. Selección de variables y filtrado de datos
vars2 <- c("price", "surface_imputado", "bathrooms", "rooms", "bedrooms",
"estudio", "parqueadero", "distancia_college")
AdaBase <- df %>% filter(grupo == "train") %>% select(all_of(vars2))
split_ada <- initial_split(AdaBase, prop = 0.8)
Ada_train <- training(split_ada)
Ada_test <- testing(split_ada)
print(dim(Ada_train))
sapply(Ada_train, function(x) length(unique(x)))
# 4. Entrenar el modelo AdaBoostM1
set.seed(123)
ada_fit <- boosting(
formula = price ~ surface_imputado + bathrooms + bedrooms,
data = Ada_train,
boos = TRUE,
mfinal = 100
)
print(dim(Ada_train))  # Revisa si hay filas y columnas
print(str(Ada_train))  # Muestra estructura de variables
Ada_train <- Ada_train %>%
mutate(
estudio = ifelse(estudio == "Sí", 1, 0),
parqueadero = ifelse(parqueadero == "Sí", 1, 0)
)
Ada_test <- Ada_test %>%
mutate(
estudio = ifelse(estudio == "Sí", 1, 0),
parqueadero = ifelse(parqueadero == "Sí", 1, 0)
)
sum(is.na(Ada_train))
sum(is.na(Ada_test))
# 4. Entrenar el modelo AdaBoostM1
set.seed(123)
ada_fit <- boosting(
formula = price ~ surface_imputado + bathrooms + bedrooms,
data = Ada_train,
boos = TRUE,
mfinal = 100
)
print(class(Ada_train))
Ada_train <- as.data.frame(Ada_train)
Ada_test <- as.data.frame(Ada_test)
ada_fit
ada_fit <- boosting(
formula = price ~ surface_imputado + bathrooms + bedrooms,
data = Ada_train,
boos = TRUE,
mfinal = 100
)
ada_fit <- boosting(price ~ ., data = Ada_train, boos = TRUE, mfinal = 100)
# Cargar las librerías listadas e instalarlas en caso de ser necesario
p_load(tidyverse, # Manipular dataframes
stringi, # Manipular cadenas de texto
rio, # Importar datos fácilmente
sf, # Leer/escribir/manipular datos espaciales
tidymodels, # entrenamiento de modelos
spatialsample, # Muestreo espacial para modelos de aprendizaje automático
rsample, # Resamplear los datos
dplyr,
parsnip, # elastic net
dials, # elastic net tunning
recipes, # Recetas
workflows,# Crear worklows RN
metrics,# Evaluación de metricas para Ml
tidymodels,#modelos
randomforest,
ranger,
rlang,
tune,
adabag,
caret)
# Cargar las librerías listadas e instalarlas en caso de ser necesario
p_load(tidyverse, # Manipular dataframes
stringi, # Manipular cadenas de texto
rio, # Importar datos fácilmente
sf, # Leer/escribir/manipular datos espaciales
tidymodels, # entrenamiento de modelos
spatialsample, # Muestreo espacial para modelos de aprendizaje automático
rsample, # Resamplear los datos
dplyr,
parsnip, # elastic net
dials, # elastic net tunning
recipes, # Recetas
workflows,# Crear worklows RN
metrics,# Evaluación de metricas para Ml
tidymodels,#modelos
randomforest,
ranger,
rlang,
tune,
adabag,
caret,
gmb)
library(gbm)
# GRADIENT BOOSTING ------------------------------------------------------------
grid_gbm<-expand.grid(n.trees= c( 50, 100,150),
interaction.depth=c(1,2),
shrinkage=c(0.01),
n.minobsinnode=c(5, 10))
vars<- c("price", "year", "surface_imputado", "rooms",
"bedrooms", "bathrooms", "property_type",
"cocina_lujo", "cocina_estandar", "parqueadero", "terraza",
"sala_comedor", "patio_lavanderia", "walkin_closet", "estudio",
"closet", "saloncomunal_recepcion", "seguridad", "piso",
"lujos", "remodelado",  "distancia_commercial",
"distancia_bank", "distancia_bus_station",
"distancia_cafe",  "distancia_college",
"distancia_hospital", "distancia_marketplace")
gmb_traim <- df %>% filter(grupo == "train") %>%  select(all_of(vars))
gmb_traim <- df %>% filter(grupo == "train") %>%  select(all_of(vars))
gbm_tree <- train(price ~ surface_imputado + bathrooms + bedrooms + parqueadero + distancia_hospital + distancia_marketplace,
data = gmb_train,
method = "gbm",
trControl = ctrl,
tuneGrid=grid_gbm,
metric = "ROC",
verbose = FALSE
)
gmb_train <- df %>% filter(grupo == "train") %>%  select(all_of(vars))
gbm_tree <- train(price ~ surface_imputado + bathrooms + bedrooms + parqueadero + distancia_hospital + distancia_marketplace,
data = gmb_train,
method = "gbm",
trControl = ctrl,
tuneGrid=grid_gbm,
metric = "ROC",
verbose = FALSE
)
ctrl <- trainControl(
method = "cv",  # Validación cruzada
number = 5,  # 5 folds
verboseIter = TRUE  # Mostrar progreso del entrenamiento
)
gbm_tree <- train(price ~ surface_imputado + bathrooms + bedrooms + parqueadero + distancia_hospital + distancia_marketplace,
data = gmb_train,
method = "gbm",
trControl = ctrl,
tuneGrid=grid_gbm,
metric = "ROC",
verbose = FALSE
)
gbm_tree <- train(price ~ surface_imputado + bathrooms + bedrooms + parqueadero + distancia_hospital + distancia_marketplace,
data = gmb_train,
method = "gbm",
trControl = ctrl,
tuneGrid=grid_gbm,
metric = "RMSE",
verbose = FALSE
)
gbm_tree
# GRADIENT BOOSTING ------------------------------------------------------------
#A. PREPROCESAMIENTO
gb_train <- df %>% filter(grupo == "train") %>%
select(price, surface_imputado, bathrooms, rooms,
bedrooms, estudio, parqueadero, distancia_college,
distancia_hospital, distancia_marketplace, terraza,
seguridad, piso, distancia_commercial, walkin_closet,
cocina_lujo, distancia_cafe, patio_lavanderia,
saloncomunal_recepcion, distancia_bank, distancia_bus_station,
year, property_type, cocina_estandar, sala_comedor, lujos, remodelado)
rec_gb <- recipe(price ~ ., data = gb_train) %>%
step_unknown(all_nominal_predictors()) %>%
step_impute_mean(all_numeric_predictors()) %>%
step_novel(all_nominal_predictors()) %>%
step_zv(all_predictors())
prepped_gb <- prep(rec_gb, training = gb_train)
gb_train <- bake(prepped_gb, new_data = gb_train)
set.seed(88)
ctrl_cv <- trainControl(
method = "cv",  # Validación cruzada
number = 5,  # 5 folds
verboseIter = TRUE
)
#B. EVALUACIÓN DE MODELOS
resultados_gbm <- data.frame(modelo = character(), MAE = numeric(), stringsAsFactors = FALSE)
for (nombre in names(formulas)) {
# Entrenar modelo con Gradient Boosting usando validación cruzada
gbm_cv <- train(
formulas[[nombre]],
data = GradientBase,
method = "gbm",
trControl = ctrl_cv,  # Activar validación cruzada
metric = "MAE",
verbose = FALSE
)
mae_val <- min(gbm_cv$results$MAE)  # Extraer el mejor MAE
resultados_gbm <- rbind(resultados_gbm, data.frame(modelo = nombre, MAE = mae_val))
}
# B. Distintas combinaciones de variables
formulas <- list(
modelo1 = price ~ surface_imputado + bathrooms + rooms + bedrooms + estudio + parqueadero + distancia_college,
modelo2 = price ~ surface_imputado + bathrooms + bedrooms + parqueadero + distancia_hospital + distancia_marketplace,
modelo3 = price ~ surface_imputado + lujos + remodelado + terraza + seguridad + piso + distancia_commercial,
modelo4 = price ~ surface_imputado + rooms + estudio + walkin_closet + cocina_lujo + distancia_cafe,
modelo5 = price ~ surface_imputado + bathrooms + patio_lavanderia + saloncomunal_recepcion + distancia_bank + distancia_bus_station,
modelo6 = price ~ year + surface_imputado + rooms + bedrooms+ bathrooms + property_type +cocina_lujo + cocina_estandar+ parqueadero+terraza+sala_comedor+patio_lavanderia+lujos
)
for (nombre in names(formulas)) {
# Entrenar modelo con Gradient Boosting usando validación cruzada
gbm_cv <- train(
formulas[[nombre]],
data = GradientBase,
method = "gbm",
trControl = ctrl_cv,  # Activar validación cruzada
metric = "MAE",
verbose = FALSE
)
mae_val <- min(gbm_cv$results$MAE)  # Extraer el mejor MAE
resultados_gbm <- rbind(resultados_gbm, data.frame(modelo = nombre, MAE = mae_val))
}
for (nombre in names(formulas)) {
# Entrenar modelo con Gradient Boosting usando validación cruzada
gbm_cv <- train(
formulas[[nombre]],
data = gb_train,
method = "gbm",
trControl = ctrl_cv,  # Activar validación cruzada
metric = "MAE",
verbose = FALSE
)
mae_val <- min(gbm_cv$results$MAE)  # Extraer el mejor MAE
resultados_gbm <- rbind(resultados_gbm, data.frame(modelo = nombre, MAE = mae_val))
}
# Ordenar modelos por MAE (mejor rendimiento primero)
resultados_gbm <- resultados_gbm[order(resultados_gbm$MAE), ]
print(resultados_gbm)
#C. PREDICCIÓN FINAL
gb_test <- df %>% filter(grupo == "test")
gb_test <- bake(prepped_gb, new_data = df_test)
gb_test <- bake(prepped_gb, new_data = gb_test)
mejor_formula <- formulas[[resultados_gbm$modelo[1]]]
mejor_formula <- formulas[[resultados_gbm$modelo[1]]]
train_matrix_final <- xgb.DMatrix(data = as.matrix(select(GB_train, all.vars(mejor_formula)[-1])), label = GB_train$price)
library(xgboost)
train_matrix_final <- xgb.DMatrix(data = as.matrix(select(GB_train, all.vars(mejor_formula)[-1])), label = GB_train$price)
train_matrix_final <- xgb.DMatrix(data = as.matrix(select(gb_train, all.vars(mejor_formula)[-1])), label = gb_train$price)
sapply(gb_train, class)
View(gb_train)
View(gb_test)
#C. PREDICCIÓN FINAL
mejor_gb <- price ~ surface_imputado + bathrooms + rooms + bedrooms + estudio + parqueadero + distancia_college
gb_test <- df %>% filter(grupo == "test")
gb_test <- bake(prepped_gb, new_data = gb_test)
test_matrix <- xgb.DMatrix(data = as.matrix(select(gb_test, all.vars(mejor_gb)[-1])))
df_test_preprocesado <- df_test_preprocesado %>%
mutate(across(where(is.factor), ~as.numeric(as.factor(.))))
df_test<- df_test %>%
mutate(across(where(is.factor), ~as.numeric(as.factor(.))))
gb_test<- gb_test %>%
mutate(across(where(is.factor), ~as.numeric(as.factor(.))))
test_matrix <- xgb.DMatrix(data = as.matrix(select(gb_test, all.vars(mejor_gb)[-1])))
# GRADIENT BOOSTING ------------------------------------------------------------
#A. PREPROCESAMIENTO
gb_train <- df %>% filter(grupo == "train") %>%
select(price, surface_imputado, bathrooms, rooms,
bedrooms, estudio, parqueadero, distancia_college,
distancia_hospital, distancia_marketplace, terraza,
seguridad, piso, distancia_commercial, walkin_closet,
cocina_lujo, distancia_cafe, patio_lavanderia,
saloncomunal_recepcion, distancia_bank, distancia_bus_station,
year, property_type, cocina_estandar, sala_comedor, lujos, remodelado)
gb_test <- df %>% filter(grupo == "test")
gb_test <- bake(prepped_gb, new_data = gb_test)
#gb_test<- gb_test %>%
#mutate(across(where(is.factor), ~as.numeric(as.factor(.))))
#test_matrix <- xgb.DMatrix(data = as.matrix(select(gb_test, all.vars(mejor_gb)[-1])))
mejor_formula <- formulas[[resultados_gbm$modelo[1]]]
gbm_final <- train(
mejor_formula,
data = GradientBase,
method = "gbm",
trControl = ctrl_cv,  # Reentrenar con validación cruzada completa
metric = "MAE",
verbose = FALSE
)
gbm_final <- train(
mejor_formula,
data = gb_test,
method = "gbm",
trControl = ctrl_cv,  # Reentrenar con validación cruzada completa
metric = "MAE",
verbose = FALSE
)
#gb_test<- gb_test %>%
#mutate(across(where(is.factor), ~as.numeric(as.factor(.))))
#test_matrix <- xgb.DMatrix(data = as.matrix(select(gb_test, all.vars(mejor_gb)[-1])))
mejor_formula <- formulas[[resultados_gbm$modelo[1]]]
gbm_final <- train(
mejor_formula,
data = gb_test,
method = "gbm",
trControl = ctrl_cv,  # Reentrenar con validación cruzada completa
metric = "MAE",
verbose = FALSE
)
gbm_final <- train(price ~ surface_imputado + bathrooms + rooms + bedrooms + estudio + parqueadero + distancia_college,
data = gb_test,
method = "gbm",
trControl = ctrl_cv,  # Reentrenar con validación cruzada completa
metric = "MAE",
verbose = FALSE
)
colSums(is.na(gb_test))
gbm_final <- train(price ~ surface_imputado + bathrooms + rooms + bedrooms + estudio + parqueadero + distancia_college,
data = gb_train,
method = "gbm",
trControl = ctrl_cv,  # Reentrenar con validación cruzada completa
metric = "MAE",
verbose = FALSE
)
predicciones_gbm <- predict(gbm_final, newdata = gb_test)
predicciones_gbm <- predict(gbm_final, newdata = gb_test)
# Asegúrate de que gb_test tenga la columna con el ID del inmueble
resultado_predicciones <- gb_test %>%
dplyr::select(property_id) %>%
dplyr::mutate(price = predicciones_gbm)
gb_test <- df %>% filter(grupo == "test")
View(gb_test)
predicciones_gbm <- predict(gbm_final, newdata = gb_test)
# Asegúrate de que gb_test tenga la columna con el ID del inmueble
resultado_predicciones <- gb_test %>%
dplyr::select(property_id) %>%
dplyr::mutate(price = predicciones_gbm)
# Exportar a CSV
write.csv(resultado_predicciones, "gb_M1.csv", row.names = FALSE)
# GRADIENT BOOSTING ------------------------------------------------------------
#A. PREPROCESAMIENTO
gb_train <- df %>% filter(grupo == "train") %>%
select(price,lat,lon, surface_imputado, bathrooms, rooms,
bedrooms, estudio, parqueadero, distancia_college,
distancia_hospital, distancia_marketplace, terraza,
seguridad, piso, distancia_commercial, walkin_closet,
cocina_lujo, distancia_cafe, patio_lavanderia,
saloncomunal_recepcion, distancia_bank, distancia_bus_station,
year, property_type, cocina_estandar, sala_comedor, lujos, remodelado)
# Cargar las librerías listadas e instalarlas en caso de ser necesario
p_load(tidyverse, # Manipular dataframes
stringi, # Manipular cadenas de texto
rio, # Importar datos fácilmente
sf, # Leer/escribir/manipular datos espaciales
tidymodels, # entrenamiento de modelos
spatialsample, # Muestreo espacial para modelos de aprendizaje automático
rsample, # Resamplear los datos
dplyr,
parsnip, # elastic net
dials, # elastic net tunning
recipes, # Recetas
workflows,# Crear worklows RN
metrics,# Evaluación de metricas para Ml
tidymodels,#modelos
randomforest,
ranger,
rlang,
tune,
adabag,
caret,
gmb,
xgboost,
spatialsample,
sf)
load("temporal.RData")
#####
gb_train <- df %>% filter(grupo == "train") %>%
select(price,lat,lon, surface_imputado, bathrooms, rooms,
bedrooms, estudio, parqueadero, distancia_college,
distancia_hospital, distancia_marketplace, terraza,
seguridad, piso, distancia_commercial, walkin_closet,
cocina_lujo, distancia_cafe, patio_lavanderia,
saloncomunal_recepcion, distancia_bank, distancia_bus_station,
year, property_type, cocina_estandar, sala_comedor, lujos, remodelado)
# Convertimos a formato espacial
gb_train_sf <- st_as_sf(
gb_train,
coords = c("lon", "lat"),  # Primero longitud, luego latitud
crs = 4326  # Sistema de referencia geográfico estándar (WGS84)
)
set.seed(88)
block_folds <- spatial_block_cv(gb_train_sf, v = 5)  # 5 bloques espaciales
resultados_gbm <- data.frame(modelo = character(), MAE = numeric(), stringsAsFactors = FALSE)
for (i in seq_along(block_folds$splits)) {
split <- block_folds$splits[[i]]
train_data <- analysis(split)  # Datos de entrenamiento del bloque
test_data <- assessment(split)  # Datos de validación del bloque
gbm_cv <- train(
price ~ surface_imputado + bathrooms + rooms + bedrooms + estudio + parqueadero + distancia_college,
data = train_data,
method = "gbm",
metric = "MAE",
verbose = FALSE
)
# Calcular error en validación
predicciones <- predict(gbm_cv, newdata = test_data)
mae_val <- mean(abs(predicciones - test_data$price))
resultados_gbm <- rbind(resultados_gbm, data.frame(modelo = paste0("Fold", i), MAE = mae_val))
}
# Mostrar resultados ordenados por MAE
resultados_gbm <- resultados_gbm[order(resultados_gbm$MAE), ]
print(resultados_gbm)
# B. Distintas combinaciones de variables
formulas <- list(
modelo1 = price ~ surface_imputado + bathrooms + rooms + bedrooms + estudio + parqueadero + distancia_college,
modelo2 = price ~ surface_imputado + bathrooms + bedrooms + parqueadero + distancia_hospital + distancia_marketplace,
modelo3 = price ~ surface_imputado + lujos + remodelado + terraza + seguridad + piso + distancia_commercial,
modelo4 = price ~ surface_imputado + rooms + estudio + walkin_closet + cocina_lujo + distancia_cafe,
modelo5 = price ~ surface_imputado + bathrooms + patio_lavanderia + saloncomunal_recepcion + distancia_bank + distancia_bus_station,
modelo6 = price ~ year + surface_imputado + rooms + bedrooms+ bathrooms + property_type +cocina_lujo + cocina_estandar+ parqueadero+terraza+sala_comedor+patio_lavanderia+lujos
)
resultados_gbm <- data.frame(modelo = character(), Fold = integer(), MAE = numeric(), stringsAsFactors = FALSE)
for (nombre in names(formulas)) {
for (i in seq_along(block_folds$splits)) {
split <- block_folds$splits[[i]]
train_data <- analysis(split)  # Datos de entrenamiento
test_data <- assessment(split)  # Datos de validación
gbm_cv <- train(
formulas[[nombre]],
data = train_data,
method = "gbm",
metric = "MAE",
verbose = FALSE
)
# Calcular MAE en el bloque de validación
predicciones <- predict(gbm_cv, newdata = test_data)
mae_val <- mean(abs(predicciones - test_data$price))
resultados_gbm <- rbind(resultados_gbm, data.frame(modelo = nombre, Fold = i, MAE = mae_val))
}
}
# Ordenar modelos por rendimiento
resultados_gbm <- resultados_gbm[order(resultados_gbm$MAE), ]
print(resultados_gbm)
mejor_modelo <- resultados_gbm %>%
group_by(modelo) %>%
summarise(MAE_promedio = mean(MAE)) %>%
arrange(MAE_promedio) %>%
slice(1) %>%
pull(modelo)
mejor_modelo <- resultados_gbm %>%
group_by(modelo) %>%
summarise(MAE_promedio = mean(MAE)) %>%
arrange(MAE_promedio)
print(mejor_modelo)
print(paste("El mejor modelo es:", mejor_modelo))
print(mejor_modelo)
# Ajustar el modelo final con todos los datos
gbm_final <- train(
formulas[["modelo6"]],
data = gb_train,
method = "gbm",
metric = "MAE",
verbose = FALSE
)
# Ajustar el modelo final con todos los datos
gbm_final <- train(
formulas[["modelo1"]],
data = gb_train,
method = "gbm",
metric = "MAE",
verbose = FALSE
)
# Predicción en datos de prueba
predicciones_gbm <- predict(gbm_final, newdata = gb_test)
# Predicción
gb_test <- df %>% filter(grupo == "test")
gb_test <- bake(prepped_gb, new_data = gb_test)
predicciones_gbm <- predict(gbm_final, newdata = gb_test)
resultado_predicciones <- gb_test %>%
dplyr::select(property_id) %>%
dplyr::mutate(price = predicciones_gbm)
write.csv(resultado_predicciones, "gb_M1.csv", row.names = FALSE)
